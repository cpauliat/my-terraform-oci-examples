


function configure_ha_services {

  # Set password for hacluster user on both nodes (if not already set)
  passwd --status hacluster | grep "Password set" > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo -e "{{ hacluster_user_password }}\n{{ hacluster_user_password }}" | passwd hacluster
    if [ $? -ne 0 ]; then
      echo "Setting password value of {{ hacluster_user_password }} for hacluster failed"
      exit 1;
    fi
  fi

  # copy ssh private key to both oss nodes.
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    corosync-keygen -l
    cp -av /etc/corosync/authkey /home/opc/authkey
    chown opc: /home/opc/authkey

    scp ${SSH_OPTIONS}  -p -3 /home/opc/authkey opc@${NODE2_IP}:/home/opc/authkey

    ssh ${SSH_OPTIONS}  opc@${NODE2_IP} "sudo mv /home/opc/authkey /etc/corosync/authkey"
    while [ $? -ne 0 ]
    do
      echo "sleeping for 20s, so NODE2 can finish deploying corosync and create directories....."
      sleep 20s
      ssh ${SSH_OPTIONS} opc@${NODE2_IP} "sudo mv /home/opc/authkey /etc/corosync/authkey"
    done
    ssh ${SSH_OPTIONS} opc@${NODE2_IP} "sudo chown root: /etc/corosync/authkey"
    rm -f /home/opc/authkey
  fi

  cp ${ha_config_dir}/corosync.conf /etc/corosync/
  chown root: /etc/corosync/corosync.conf
  sed -i "s/LOCAL_NODE_IP/${LOCAL_NODE_IP}/" /etc/corosync/corosync.conf

  # Look for entire line and replace, so it does not replace other string whcih has substring  NODE1_xxxx
  #sed -i "s/name: NODE1/name: ${NODE1}/" /etc/corosync/corosync.conf
  #sed -i "s/name: NODE2/name: ${NODE2}/" /etc/corosync/corosync.conf


  sed -i "s/NODE1_IP/${NODE1_IP}/" /etc/corosync/corosync.conf
  sed -i "s/NODE2_IP/${NODE2_IP}/" /etc/corosync/corosync.conf
  
  sed -i "s/NODE1_VNIC2_IP/${NODE1_VNIC2_IP}/" /etc/corosync/corosync.conf
  sed -i "s/NODE2_VNIC2_IP/${NODE2_VNIC2_IP}/" /etc/corosync/corosync.conf
      
  sed -i "s/QUORUM_NODE/${QUORUM_NODE}/" /etc/corosync/corosync.conf

  # Only if it ends in NODE1 or NODE2
  sed -i "s/NODE1$/${NODE1}/" /etc/corosync/corosync.conf
  sed -i "s/NODE2$/${NODE2}/" /etc/corosync/corosync.conf

    
  mkdir -p /etc/corosync/service.d/
  cp -f ${ha_config_dir}/pcmk /etc/corosync/service.d/pcmk
  chown root: /etc/corosync/service.d/pcmk

  mv -vn /etc/sysconfig/corosync /etc/sysconfig/corosync.orig
  cp -f ${ha_config_dir}/corosync  /etc/sysconfig/corosync
  chown root: /etc/sysconfig/corosync

  mkdir -p /var/log/corosync/
  # Primary reason for this while loop, is to ensure node2 get the authkey file from node1. Until then, Node2 should not start corosync service.
  if [ "$LOCAL_NODE" = "$NODE2" ]; then
    while ( ! [ -f /etc/corosync/authkey ] )
    do
      echo "wait for /etc/corosync/authkey to get transfer from node1"
      sleep 5s
    done
  fi

  systemctl start corosync
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  corosync  | grep "(running)" ' ;
    while ( [ $? -ne 0 ] )
    do
      echo "waiting for corosync to come online on node2"
      sleep 10
      ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  corosync  | grep "(running)" ' ;
    done
  fi

  corosync-cmapctl | grep members


  # Configuring PCSD
  systemctl start pcsd.service
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  pcsd  | grep "(running)" ' ;
    while ( [ $? -ne 0 ] )
    do
      echo "waiting for pcsd.service to come online on node2"
      sleep 10
      ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo systemctl status  pcsd  | grep "(running)" ' ;
    done
  fi

  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    echo "{{ hacluster_user_password }}" | pcs cluster auth --name nfs_cluster ${NODE1} ${NODE2} -u hacluster

    echo "{{ hacluster_user_password }}" | pcs cluster auth --name nfs_cluster ${QUORUM_NODE} -u hacluster

  fi

}

function configure_nfs_server {

  #exportfs_client_spec_1_cidr_block="10.0.3.0/24"
  #exportfs_client_spec_2_cidr_block="10.0.0.0/24"

  exportfs_client_spec_1_cidr_block="{{ private_storage_subnet_cidr_block }}"
  exportfs_client_spec_2_cidr_block="{{ public_subnet_cidr_block }}"
  exportfs_client_spec_1_netmask=`ipcalc -m  $exportfs_client_spec_1_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_1_network=`ipcalc -n  $exportfs_client_spec_1_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_2_netmask=`ipcalc -m  $exportfs_client_spec_2_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_2_network=`ipcalc -n  $exportfs_client_spec_2_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_1="${exportfs_client_spec_1_network}/${exportfs_client_spec_1_netmask}"
  exportfs_client_spec_2="${exportfs_client_spec_2_network}/${exportfs_client_spec_2_netmask}"



  if [ "{{ storage_server_dual_nics }}" = "true" ]; then
    exportfs_client_spec_3_cidr_block="{{ private_fs_subnet_cidr_block }}"
    exportfs_client_spec_3_netmask=`ipcalc -m  $exportfs_client_spec_3_cidr_block | gawk -F "=" '{ print $2}'`
    exportfs_client_spec_3_network=`ipcalc -n  $exportfs_client_spec_3_cidr_block | gawk -F "=" '{ print $2}'`
    exportfs_client_spec_3="${exportfs_client_spec_3_network}/${exportfs_client_spec_3_netmask}"
    
  fi


  # /mnt/nfsshare/exports   172.28.16.0/255.255.240.0(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,insecure_locks,acl,no_pnfs,fsid=0,anonuid=65534,anongid=65534,sec=sys,rw,secure,no_root_squash,no_all_squash)



    if [ "$use_uhp" = "true" ]; then
      filesystem_dir=$fs0_dir
      nfs_root_dir="${filesystem_dir}/exports"
      exports_line1="${nfs_root_dir} ${exportfs_client_spec_1}(${options})"
      exports_line2="${nfs_root_dir} ${exportfs_client_spec_2}(${options})"
      echo "$exports_line1" >> /etc/exports
      echo "$exports_line2" >> /etc/exports
      if [ "{{ storage_server_dual_nics }}" = "true" ]; then
        exports_line3="${nfs_root_dir} ${exportfs_client_spec_3}(${options})"
        echo "$exports_line3" >> /etc/exports
      fi
      
    fi

    if [ "$use_non_uhp_fs1" = "true" ]; then
      filesystem_dir=$fs1_dir
      nfs_root_dir="${filesystem_dir}/exports"
      exports_line1="${nfs_root_dir} ${exportfs_client_spec_1}(${options})"
      exports_line2="${nfs_root_dir} ${exportfs_client_spec_2}(${options})"
      echo "$exports_line1" >> /etc/exports
      echo "$exports_line2" >> /etc/exports
      if [ "{{ storage_server_dual_nics }}" = "true" ]; then
        exports_line3="${nfs_root_dir} ${exportfs_client_spec_3}(${options})"
        echo "$exports_line3" >> /etc/exports
      fi
      
    fi

    if [ "$use_non_uhp_fs2" = "true" ]; then
      filesystem_dir=$fs2_dir
      nfs_root_dir="${filesystem_dir}/exports"
      exports_line1="${nfs_root_dir} ${exportfs_client_spec_1}(${options})"
      exports_line2="${nfs_root_dir} ${exportfs_client_spec_2}(${options})"
      echo "$exports_line1" >> /etc/exports
      echo "$exports_line2" >> /etc/exports
      if [ "{{ storage_server_dual_nics }}" = "true" ]; then
        exports_line3="${nfs_root_dir} ${exportfs_client_spec_3}(${options})"
        echo "$exports_line3" >> /etc/exports
      fi
      
    fi

    if [ "$use_non_uhp_fs3" = "true" ]; then
      filesystem_dir=$fs3_dir
      nfs_root_dir="${filesystem_dir}/exports"
      exports_line1="${nfs_root_dir} ${exportfs_client_spec_1}(${options})"
      exports_line2="${nfs_root_dir} ${exportfs_client_spec_2}(${options})"
      echo "$exports_line1" >> /etc/exports
      echo "$exports_line2" >> /etc/exports
      if [ "{{ storage_server_dual_nics }}" = "true" ]; then
        exports_line3="${nfs_root_dir} ${exportfs_client_spec_3}(${options})"
        echo "$exports_line3" >> /etc/exports
      fi
      
    fi


  # Setup rsync to copy below files from Node1 to Node2, only if Node1 has a more updated file.  This assumes that Node1 will be used for all manual updates to the below files.
  # Configure nodes to allow ssh for root user on both nodes.
  mv /root/.ssh/authorized_keys /root/.ssh/authorized_keys.backup
  cp /home/opc/.ssh/authorized_keys /root/.ssh/authorized_keys
  cp /home/opc/.ssh/id_rsa /root/.ssh/id_rsa

  # cronjob to rsync files every 1 minute from Node1 to Node2.
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    echo  "* * * * * /bin/rsync -avziu -e ssh  /etc/exports root@${NODE2}:/etc/exports  "  >> /var/spool/cron/root
    echo  "* * * * * /bin/rsync -avziu -e ssh  /etc/projid root@${NODE2}:/etc/projid  "  >> /var/spool/cron/root
    echo  "* * * * * /bin/rsync -avziu -e ssh  /etc/projects root@${NODE2}:/etc/projects  "  >> /var/spool/cron/root
    # tail -f /var/log/cron
  fi
} # end of function configure_nfs_server


function configure_vip_move {

  # use Instance Principal for auth
  # Configuring OCI-CLI
  mkdir -p /home/oracle-cli/
  chown root: /home/oracle-cli/
  chmod 755 /home/oracle-cli/
  wget https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh
  bash install.sh --accept-all-defaults --exec-dir /home/oracle-cli/bin/ --install-dir /home/oracle-cli/lib/
  rm -f install.sh
  rm -rf /root/bin/oci-cli-scripts
  mkdir -p /home/oracle-cli/.oci
  chown hacluster:haclient /home/oracle-cli/.oci
  chmod 700 /home/oracle-cli/.oci
  /home/oracle-cli/bin/oci os ns get --auth instance_principal

  cp ${ha_config_dir}/move_secip.sh /home/oracle-cli/move_secip.sh
  chmod +x /home/oracle-cli/move_secip.sh
  chmod 700 /home/oracle-cli/move_secip.sh
  chown hacluster:haclient /home/oracle-cli/move_secip.sh
  cp -f /root/env_variables.sh /home/oracle-cli/
  chown hacluster:haclient /home/oracle-cli/env_variables.sh

  cp ${ha_config_dir}/ip_move.sh   /var/lib/pacemaker/ip_move.sh
  chown root: /var/lib/pacemaker/ip_move.sh
  chmod 0755 /var/lib/pacemaker/ip_move.sh
  sed -i "s|IPADDR2_VIP_NAME|${IPADDR2_VIP_NAME}|g" /var/lib/pacemaker/ip_move.sh
  touch /var/log/pacemaker_ip_move.log
  chown hacluster:haclient /var/log/pacemaker_ip_move.log

} # end of function configure_vip_move


function configure_stonith {
  
  # Start - Stonith SBD fencing config
  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    timeout_watchdog=20
    timeout_msgwait=$(( 2 * $timeout_watchdog))
    warn_threshold_watchdog_timeout=$(( $timeout_watchdog - 3 ))
    echo  $timeout_watchdog $timeout_msgwait $warn_threshold_watchdog_timeout

    sbd -d /dev/oracleoci/oraclevdb -4 $timeout_msgwait -1 $timeout_watchdog -5 $warn_threshold_watchdog_timeout  create

    #sbd -d /dev/oracleoci/oraclevdb create
    # I should see the below info
    sbd -d /dev/oracleoci/oraclevdb dump

    ##==Dumping header on disk /dev/oracleoci/oraclevdb
    ##Header version     : 2.1
    ##UUID               : d484107e-3491-4a1d-b1ce-ea0a2ee2b649
    ##Number of slots    : 255
    ##Sector size        : 512
    ##Timeout (watchdog) : 20
    ##Timeout (allocate) : 2
    ##Timeout (loop)     : 1
    ##Timeout (msgwait)  : 40
    ##==Header on disk /dev/oracleoci/oraclevdb is dumped

  fi

  echo softdog > /etc/modules-load.d/watchdog.conf
  systemctl restart systemd-modules-load
  lsmod | egrep "(wd|dog)"

  sed -i "s/#SBD_DEVICE=.*/SBD_DEVICE=\"\/dev\/oracleoci\/oraclevdb\"/g"  /etc/sysconfig/sbd
  sed -i "s/SBD_OPTS=/SBD_OPTS=\"-W\"/g" /etc/sysconfig/sbd
  less /etc/sysconfig/sbd |  egrep "SBD_OPTS|SBD_DEVICE|SBD_DELAY_START"

  systemctl enable sbd
  # End - Stonith SBD fencing config

}


function configure_pcs_cluster {

  cd /root;
  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    # NOTE: With Stonith SBD fencing - first time starting cluster using pcs cluster start --all fails, but subsequent calls works. Similarly sometimes first time call to "pcs cluster start" fails, but subsequent calls work.  Hence the while loop.  "Don't know why.  Have reached LINBIT community.
    pcs cluster start
    while ( [ $? -ne 0 ] )
    do
      sudo pcs cluster stop --force
      echo "waiting for pcs cluster start to come online on $NODE1"
      sleep 15
      pcs cluster start
    done

    sleep 10s

    ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo pcs cluster start' ;
    while ( [ $? -ne 0 ] )
    do
      sudo pcs cluster stop --force
      echo "waiting for pcs cluster start to come online on $NODE2"
      sleep 10
      ssh ${SSH_OPTIONS} opc@${NODE2_IP}  'sudo pcs cluster start' ;
    done
    sleep 15s;

    pcs status

    pcs cluster cib /root/${pcs_cfg}

    # Quorum node changes
    echo "starting quorum changes..."
    pcs -f /root/${pcs_cfg} quorum device add model net host=${QUORUM_NODE} algorithm=ffsplit
    pcs -f /root/${pcs_cfg}  quorum config
    pcs -f /root/${pcs_cfg}  quorum status
    pcs -f /root/${pcs_cfg}  quorum device status
    echo "ending quorum changes."
    # end

    pcs -f /root/${pcs_cfg} property set stonith-enabled=true
    # In 2 nodes, there is no quorum. hence no-quorum-policy=ignore  instead of no-quorum-policy=stop
    # With 3rd Quorum node - set no-quorum-policy=stop
    pcs -f /root/${pcs_cfg} property set no-quorum-policy=stop

    # If migration-threshold=INFINITY, a systemctl stop nfs-server will try to keep restarting the service on the same node instead of failover to other node.
    # migration-threshold=1 - means after 1 failure, it will failover.
    # LINBIT recommended using 3, so failover does not happen if someone accidently does stop nfs-server outside pcs.
    pcs -f /root/${pcs_cfg} resource defaults migration-threshold=3
    pcs -f /root/${pcs_cfg} resource defaults resource-stickiness=0
    # LINBIT - to automatically cleanup failed monitors.  Wait for sometime, like 15m.  If the monitor is not continuously failing, then its okay for them to be cleaned up automatically.
    pcs -f /root/${pcs_cfg} resource defaults failure-timeout=15m

    # default timeout in PCS is 20s, sometimes it takes more than 20s for stonith monitor calls, hence make it 50s.
    # default Stonith monitor interval=60s
    # sudo pcs stonith show --full
    # Set it to be "timeout_msgwait + 5" seconds. eg: 40+5=45
    power_timeout=$(( $timeout_msgwait + 5 ))
    pcs -f /root/${pcs_cfg} stonith create sbd_fencing_$NODE1  fence_sbd devices=/dev/oracleoci/oraclevdb pcmk_host_list=$NODE1 pcmk_delay_base=0 power_timeout=$power_timeout op monitor timeout=50s
    # A 5 seconds delay was added to NODE2 to ensure there is no race condition - fence-race.
    pcs -f /root/${pcs_cfg} stonith create sbd_fencing_$NODE2  fence_sbd devices=/dev/oracleoci/oraclevdb pcmk_host_list=$NODE2 pcmk_delay_base=5 power_timeout=$power_timeout op monitor timeout=50s

    pcs -f /root/${pcs_cfg} constraint location sbd_fencing_$NODE1 avoids $NODE1
    pcs -f /root/${pcs_cfg} constraint location sbd_fencing_$NODE2 avoids $NODE2

  fi
  

}



function add_lvm_activate_resources {

  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    if [ "$use_uhp" = "true" ]; then
        lv_name=$fs0_lv_name
        vg_name=$fs0_vg_name
        pcs -f /root/${pcs_cfg} resource create $lv_name ocf:heartbeat:LVM-activate vgname=$vg_name vg_access_mode=system_id $pcs_group_parameter
    fi

    if [ "$use_non_uhp_fs1" = "true" ]; then
        lv_name=$fs1_lv_name
        vg_name=$fs1_vg_name
        pcs -f /root/${pcs_cfg} resource create $lv_name ocf:heartbeat:LVM-activate vgname=$vg_name vg_access_mode=system_id $pcs_group_parameter
    fi

    if [ "$use_non_uhp_fs2" = "true" ]; then
        lv_name=$fs2_lv_name
        vg_name=$fs2_vg_name
        pcs -f /root/${pcs_cfg} resource create $lv_name ocf:heartbeat:LVM-activate vgname=$vg_name vg_access_mode=system_id $pcs_group_parameter
    fi

    if [ "$use_non_uhp_fs3" = "true" ]; then
        lv_name=$fs3_lv_name
        vg_name=$fs3_vg_name
        pcs -f /root/${pcs_cfg} resource create $lv_name ocf:heartbeat:LVM-activate vgname=$vg_name vg_access_mode=system_id $pcs_group_parameter
    fi
    
  fi
  
}


function add_filesystem_resources {

  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    if [ "$use_uhp" = "true" ]; then
        lv_name=$fs0_lv_name
        vg_name=$fs0_vg_name
        filesystem_name=$fs0_name
        filesystem_device="/dev/${vg_name}/${lv_name}"
        filesystem_dir=$fs0_dir
        filesystem_type="xfs"
        options="noatime,inode64,uquota,prjquota"

        pcs -f /root/${pcs_cfg} resource create ${filesystem_name} Filesystem \
        device="${filesystem_device}" \
        directory="${filesystem_dir}" \
        fstype="${filesystem_type}" \
        options="${options}" \
        $pcs_group_parameter

        pcs -f /root/${pcs_cfg} resource update ${filesystem_name} op stop timeout=200s

    fi

    if [ "$use_non_uhp_fs1" = "true" ]; then
        lv_name=$fs1_lv_name
        vg_name=$fs1_vg_name
        filesystem_name=$fs1_name
        filesystem_device="/dev/${vg_name}/${lv_name}"
        filesystem_dir=$fs1_dir
        filesystem_type="xfs"
        options="noatime,inode64,uquota,prjquota"

        pcs -f /root/${pcs_cfg} resource create ${filesystem_name} Filesystem \
        device="${filesystem_device}" \
        directory="${filesystem_dir}" \
        fstype="${filesystem_type}" \
        options="${options}" \
        $pcs_group_parameter

        pcs -f /root/${pcs_cfg} resource update ${filesystem_name} op stop timeout=200s
    fi

    if [ "$use_non_uhp_fs2" = "true" ]; then
        lv_name=$fs2_lv_name
        vg_name=$fs2_vg_name
        filesystem_name=$fs2_name
        filesystem_device="/dev/${vg_name}/${lv_name}"
        filesystem_dir=$fs2_dir
        filesystem_type="xfs"
        options="noatime,inode64,uquota,prjquota"

        pcs -f /root/${pcs_cfg} resource create ${filesystem_name} Filesystem \
        device="${filesystem_device}" \
        directory="${filesystem_dir}" \
        fstype="${filesystem_type}" \
        options="${options}" \
        $pcs_group_parameter

        pcs -f /root/${pcs_cfg} resource update ${filesystem_name} op stop timeout=200s
    fi

    if [ "$use_non_uhp_fs3" = "true" ]; then
        lv_name=$fs3_lv_name
        vg_name=$fs3_vg_name
        filesystem_name=$fs3_name
        filesystem_device="/dev/${vg_name}/${lv_name}"
        filesystem_dir=$fs3_dir
        filesystem_type="xfs"
        options="noatime,inode64,uquota,prjquota"

        pcs -f /root/${pcs_cfg} resource create ${filesystem_name} Filesystem \
        device="${filesystem_device}" \
        directory="${filesystem_dir}" \
        fstype="${filesystem_type}" \
        options="${options}" \
        $pcs_group_parameter

        pcs -f /root/${pcs_cfg} resource update ${filesystem_name} op stop timeout=200s
    fi

  fi
  
}




function configure_nfsgroup_nfs_resources {

  if [ "$LOCAL_NODE" = "$NODE1" ]; then

    service_name=$1
    filesystem_dir=$2
    ipaddr2_vip_name=$3
    TARGET_VIP=$4
    cidr_netmask=$5
    pcs -f /root/${pcs_cfg} resource create ${service_name}  nfsserver \
    nfs_shared_infodir=${filesystem_dir}/nfsinfo nfs_no_notify=true \
    $pcs_group_parameter

    pcs -f /root/${pcs_cfg} resource update ${service_name} op stop timeout=90s
    pcs -f /root/${pcs_cfg} resource update ${service_name} op monitor interval=20s timeout=40s


    pcs -f /root/${pcs_cfg} resource create ${ipaddr2_vip_name} ocf:heartbeat:IPaddr2 ip=${TARGET_VIP} cidr_netmask=${cidr_netmask} op monitor interval=20s $pcs_group_parameter

    pcs -f /root/${pcs_cfg} alert create id=ip_move description="Move IP address using oci-cli" path=/var/lib/pacemaker/ip_move.sh
    pcs -f /root/${pcs_cfg} alert recipient add ip_move id=logfile_ip_move value=/var/log/pacemaker_ip_move.log

  fi

}


#######
# Start of script
#######

SSH_OPTIONS=" -i /home/opc/.ssh/id_rsa -o BatchMode=yes -o StrictHostkeyChecking=no "
MDATA_VNIC_URL="http://169.254.169.254/opc/v1/vnics/"
pcs_cfg="nfs_cfg"


use_uhp={{ use_uhp }}
use_non_uhp_fs1={{ use_non_uhp_fs1 }}
use_non_uhp_fs2={{ use_non_uhp_fs2 }}
use_non_uhp_fs3={{ use_non_uhp_fs3 }}
fs0_name="{{ fs0_name }}"
fs1_name="{{ fs1_name }}"
fs2_name="{{ fs2_name }}"
fs3_name="{{ fs3_name }}"
fs0_lv_name="{{ fs0_lv_name }}"
fs0_vg_name="{{ fs0_vg_name }}"
fs1_lv_name="{{ fs1_lv_name }}"
fs1_vg_name="{{ fs1_vg_name }}"
fs2_lv_name="{{ fs2_lv_name }}"
fs2_vg_name="{{ fs2_vg_name }}"
fs3_lv_name="{{ fs3_lv_name }}"
fs3_vg_name="{{ fs3_vg_name }}"
fs0_dir="{{ fs0_dir }}"
fs1_dir="{{ fs1_dir }}"
fs2_dir="{{ fs2_dir }}"
fs3_dir="{{ fs3_dir }}"


###lv_name="{{ lv_name }}"
###vg_name="{{ vg_name }}"
###filesystem_name="nfsshare"
###filesystem_type="xfs"
###filesystem_device="/dev/{{ vg_name }}/{{ lv_name }}"
###filesystem_dir="/mnt/nfsshare"


options="rw,sync,no_root_squash,no_all_squash,no_subtree_check,insecure_locks"
###nfs_root_dir="${filesystem_dir}/exports"

ipaddr2_vip_name="{{ ipaddr2_vip_name }}"
service_name="nfs-daemon"

ha_config_dir="/home/opc/ha_config"

group_name="nfsgroup"
pcs_group_parameter=" --group ${group_name}"

  echo "Entering Shared Disk HA Script"

  #Deploy components to implement HA for Storage Service

  LOCAL_NODE=`hostname`;
  LOCAL_NODE_IP=`nslookup $LOCAL_NODE | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  NODE1="{{ storage_server_hostname_prefix }}1" ;
  NODE2="{{ storage_server_hostname_prefix }}2" ;
  NODE1_IP=`nslookup $NODE1 | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  NODE2_IP=`nslookup $NODE2 | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  NODE1_FQDN="{{ storage_server_hostname_prefix }}1.{{ storage_subnet_domain_name }}" ;
  NODE2_FQDN="{{ storage_server_hostname_prefix }}2.{{ storage_subnet_domain_name }}" ;
  echo "$NODE1_IP $NODE1_FQDN $NODE1" >> /etc/hosts
  echo "$NODE2_IP $NODE2_FQDN $NODE2" >> /etc/hosts
  # VIRTUAL IP
  TARGET_VIP={{ nfs_server_ip }}

  QUORUM_NODE="{{ quorum_server_hostname }}"
  
  if [ "{{ storage_server_dual_nics }}" = "true" ]; then
    NODE1_VNIC2_HOSTNAME="{{ storage_server_filesystem_vnic_hostname_prefix }}1" ;
    NODE2_VNIC2_HOSTNAME="{{ storage_server_filesystem_vnic_hostname_prefix }}2" ;
    NODE1_VNIC2_IP=`nslookup $NODE1_VNIC2_HOSTNAME | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
    NODE2_VNIC2_IP=`nslookup $NODE2_VNIC2_HOSTNAME | grep "Address: " | grep -v "#" | gawk '{print $2}'` ;
  fi
  

  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    if [ "{{ storage_server_dual_nics }}" = "true" ]; then
      node1vnic=`curl -s $MDATA_VNIC_URL | jq '.[1].vnicId' | sed 's/"//g' ` ;
    else
      node1vnic=`curl -s $MDATA_VNIC_URL | jq '.[0].vnicId' | sed 's/"//g' ` ;
    fi

    ssh ${SSH_OPTIONS}  opc@${NODE2_IP} "ls -l /home/opc/.ssh/id_rsa"
    while [ $? -ne 0 ]
    do
      echo "wait for TF scripts to copy ssh keys..."
      sleep 5s
      ssh ${SSH_OPTIONS}  opc@${NODE2_IP} "ls -l /home/opc/.ssh/id_rsa"
    done

    if [ "{{ storage_server_dual_nics }}" = "true" ]; then
      node2vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE2_IP} "curl -s $MDATA_VNIC_URL | jq '.[1].vnicId'  "` ;
    else
      node2vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE2_IP} "curl -s $MDATA_VNIC_URL | jq '.[0].vnicId'  "` ;
    fi
    node2vnic=`echo $node2vnic_w_quotes |  sed 's/"//g' ` ;
  else
    # SWAP logic, since its node2 here.
    if [ "{{ storage_server_dual_nics }}" = "true" ]; then
      node2vnic=`curl -s $MDATA_VNIC_URL | jq '.[1].vnicId' | sed 's/"//g' ` ;
    else
      node2vnic=`curl -s $MDATA_VNIC_URL | jq '.[0].vnicId' | sed 's/"//g' ` ;
    fi


    ssh ${SSH_OPTIONS}  opc@${NODE1_IP} "ls -l /home/opc/.ssh/id_rsa"
    while [ $? -ne 0 ]
    do
      echo "wait for TF scripts to copy ssh keys..."
      sleep 5s
      ssh ${SSH_OPTIONS}  opc@${NODE1_IP} "ls -l /home/opc/.ssh/id_rsa"
    done

    if [ "{{ storage_server_dual_nics }}" = "true" ]; then
      node1vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE1_IP} "curl -s $MDATA_VNIC_URL | jq '.[1].vnicId'  "` ;
    else
      node1vnic_w_quotes=`ssh ${SSH_OPTIONS} opc@${NODE1_IP} "curl -s $MDATA_VNIC_URL | jq '.[0].vnicId'  "` ;
    fi
    node1vnic=`echo $node1vnic_w_quotes |  sed 's/"//g' ` ;
  fi


  if [ "{{ storage_server_dual_nics }}" = "true" ]; then
    subnetCidrBlock=`curl -s $MDATA_VNIC_URL | jq '.[1].subnetCidrBlock  ' | sed 's/"//g' ` ;
  else
    subnetCidrBlock=`curl -s $MDATA_VNIC_URL | jq '.[0].subnetCidrBlock  ' | sed 's/"//g' ` ;
  fi
  cidr_netmask=`echo $subnetCidrBlock | gawk -F"/" '{ print $2 }'` ;



  echo "
  LOCAL_NODE=\"${LOCAL_NODE}\"
  LOCAL_NODE_IP=\"${LOCAL_NODE_IP}\"
  NODE1=\"${NODE1}\"
  NODE2=\"${NODE2}\"
  NODE1_IP=\"${NODE1_IP}\"
  NODE2_IP=\"${NODE2_IP}\"
  NODE1_FQDN=\"${NODE1_FQDN}\"
  NODE2_FQDN=\"${NODE2_FQDN}\"
  TARGET_VIP=\"${TARGET_VIP}\"
  node1vnic=\"${node1vnic}\"
  node2vnic=\"${node2vnic}\"
  QUORUM_NODE=\"${QUORUM_NODE}\"
  " > /root/env_variables.sh

  if [ "{{ storage_server_dual_nics }}" = "true" ]; then
    echo "
    NODE1_VNIC2_HOSTNAME=\"${NODE1_VNIC2_HOSTNAME}\"
    NODE2_VNIC2_HOSTNAME=\"${NODE2_VNIC2_HOSTNAME}\"
    NODE1_VNIC2_IP=\"${NODE1_VNIC2_IP}\"
    NODE2_VNIC2_IP=\"${NODE2_VNIC2_IP}\"
    " >> /root/env_variables.sh
  fi
  echo "source /root/env_variables.sh" >>  /root/.bash_profile


  # call functions

  configure_ha_services
  configure_vip_move

  configure_nfs_server

  systemctl enable pcsd;
  systemctl enable pacemaker;
  systemctl enable corosync;

  configure_stonith
  configure_pcs_cluster

  add_lvm_activate_resources
  add_filesystem_resources

  #configure_nfsgroup_filesystem_resources "$lv_name" "$vg_name" "$filesystem_name"  "$filesystem_device"  "$filesystem_dir" "$filesystem_type"

  # Since there is only 1 deamon running, pick one filesystem_dir.
  if [ "$use_uhp" = "true" ]; then
    filesystem_dir=$fs0_dir
  fi
  if [ "$use_non_uhp_fs1" = "true" ]; then
    filesystem_dir=$fs1_dir
  fi

  configure_nfsgroup_nfs_resources "$service_name" "$filesystem_dir" "$ipaddr2_vip_name" "$TARGET_VIP" "$cidr_netmask"


  # Now push the configuration so it becomes active.
  if [ "$LOCAL_NODE" = "$NODE1" ]; then
    pcs cluster cib-push /root/${pcs_cfg}
    echo "sleeping for 25 seconds for PCS cluster cib-push /root/${pcs_cfg} to complete"
    sleep 25s

    pcs status
    pcs stonith sbd status --full
    pcs stonith show --full
    pcs property show
    pcs resource show --full
    pcs quorum status
    pcs stonith list
    #pcs stonith describe  fence_sbd
    pcs property --all  | egrep "stonith-|watchdog"

    # Changes for Quorum node
    # corosync needs to be stop for the below to work.
    pcs cluster stop --all
    sleep 5s
    pcs quorum update wait_for_all=0
    sleep 25s
    pcs cluster start --all
    # heuristics change not applied - Hard to validate if it makes a difference to quorum decision making.
    ##pcs quorum device update heuristics mode=on exec_CUSTOM=/root/heuristics.sh
    # good to do a final restart of cluster services
    ##sleep 10s
    ##pcs cluster stop --all
    ##sleep 25s
    ##pcs cluster start --all

  else
    # for node1 to finish the configuration
    echo "Waiting for node1 to finish configuring PCS. You can run pcs status to see the latest status"
    sleep 60s
  fi






