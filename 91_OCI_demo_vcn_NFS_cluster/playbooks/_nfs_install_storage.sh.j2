

make_filesystem()
{
    lvcreate -l 100%VG --stripes $lvm_stripes_cnt --stripesize "{{ block_size }}K" -n $disk_name $vg_nfs_name
    lvdisplay

    # Create XFS filesystem with Inodes set at 512 and Directory block size at 8192
    # and set the su and sw for optimal stripe performance
    # lvm_stripe_size is assumed to be in KB, hence multiply by 1024 to convert to bytes.
    # su must be a multiple of the sector size (4096)
    # sw must be equal to # of disk within RAID or LVM.
    su=$(({{ block_size }}*1024)) ;  echo $su
    sw=$((lvm_stripes_cnt)) ; echo $sw
    mkfs.xfs -f -i size=512 -n size=8192 -d su=${su},sw=${sw} /dev/${vg_nfs_name}/${disk_name}
    ###mount_point="/mnt/nfsshare"
    mkdir -p ${mount_point}
    # Temporary mount to create directories needed by ocf::heartbeat:exportfs
    mount -t xfs -o noatime,inode64,uquota,prjquota /dev/${vg_nfs_name}/${disk_name} ${mount_point}
    df -h
    mkdir -p ${mount_point}/exports
    if [ "{{ fs_ha }}" = "true" ]; then
      # unmount it, since we want PCS to manage it.
      umount ${mount_point}
    else
      # https://docs.cloud.oracle.com/en-us/iaas/Content/Block/References/fstaboptionsconsistentdevicepaths.htm#fstab_Options_for_Block_Volumes_Using_Consistent_Device_Paths
      # https://askubuntu.com/questions/9939/what-do-the-last-two-fields-in-fstab-mean
      # https://docs.cloud.oracle.com/en-us/iaas/Content/Block/References/fstaboptions.htm
      echo "Keep the disk mounted."
      echo "/dev/${vg_nfs_name}/${disk_name}  ${mount_point}   xfs     defaults,_netdev,nofail,noatime,inode64,uquota,prjquota        0 2" >> /etc/fstab
    fi
}

create_volume_group() {

  # Required to control and support lvm activate via PCS.
  # https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs
  # Update of /etc/lvm/lvm.conf moved to ansible code.
  ##sed -i 's/system_id_source = \"none\"/system_id_source = \"uname\"/g' /etc/lvm/lvm.conf
  # Do the below on each node, if manually required. 
  #vgchange --systemid nfs-1 vg_nfs_disk
  dataalignment=$(({{ block_size }})); echo $dataalignment;
  #pvcreate --dataalignment $dataalignment  $disk
  pvcreate  $disk
  pvdisplay
  physicalextentsize="{{ block_size }}K";  echo $physicalextentsize
  if [ $disk_counter -eq 1  ]; then
    #vgcreate  --physicalextentsize $physicalextentsize $vg_nfs_name $disk
    vgcreate $vg_nfs_name $disk
  else
    vgextend $vg_nfs_name $disk
  fi
  vgdisplay

}

process_disks()
{
  disk_lst=$1
  disk_name=$2
  vg_nfs_name="vg_nfs_${disk_name}"
  disk_counter=1
  count=1
  # Configure physical volumes and volume group
  for disk in $disk_lst
  do
    create_volume_group
    disk_counter=$((disk_counter+1))
    count=$((count+1))
  done

  lvm_stripes_cnt=$((disk_counter-1))
  make_filesystem 
}


find_UHP_block_volume()
{

  blk_lst=""  ;  blk_cnt=0
  # For iSCSI UHP, block plugin will do batch iscsi login or logout for multipath. Its not done by TF code
  while [ ! -e /dev/mapper/mpatha ]
  do
    sleep 60s
    echo "Waiting for UHP block plugin to do batch iscsi login to complete ..."
  done

  if [ "{{ fs_ha }}" = "true" ]; then
      if [ "{{ use_uhp }}" = "true" ]; then
          blk_lst=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$" | grep "vdc$" | sort )
          blk_cnt=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$" | grep "vdc$" | wc -l)
      else
          blk_lst=""
          blk_cnt=0
      fi
  else
      if [ "{{ use_uhp }}" = "true" ]; then
          blk_lst=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme |  grep "vdb$" | sort )
          blk_cnt=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme |  grep "vdb$" | wc -l)
      else
          blk_lst=""
          blk_cnt=0
      fi
  fi
}
      

# pass 2 parameters -  $1 - start position and $2 - disk_count
find_block_volumes()
{
  blk_lst=""  ;  blk_cnt=0
  # Wait for block-attach of the Block volumes to complete. Terraform then creates the below file on server nodes of cluster.
  while [ ! -f /tmp/block-attach.complete ]
  do
    sleep 60s
    echo "Waiting for block-attach via Terraform to  complete ..."
  done

  start_position=$1 ; disk_count=$2
  #if [ $start_position -eq 1 ]; then
  #    head=$start_position
  #    tail=$disk_count
  #else
      head=$((disk_count + start_position - 1 ))
      tail=$disk_count
  #fi
  echo "head -n $head | tail -n $tail "
  if [ "{{ fs_ha }}" = "true" ]; then
      if [ "{{ use_uhp }}" = "true" ]; then
      
          # Exclude, /dev/oracleoci/oraclevdb from the list, since it will be used for Stonith fencing.
          # Exclude, /dev/oracleoci/oraclevdc from the list, since it will be used for UHP volume.
          # Gather rest of the block devices
          # blk_lst will be full path, eg  /dev/oracleoci/oraclevdd , etc
          blk_lst=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$|vdc$" | sort | head -n $head | tail -n $tail)
          blk_cnt=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$|vdc$" | head -n $head | tail -n $tail | wc -l)
      else
      
          # Exclude, /dev/oracleoci/oraclevdb from the list, since it will be used for Stonith fencing.
          # Gather rest of the block devices
          # blk_lst will be full path, eg  /dev/oracleoci/oraclevdd , etc
          blk_lst=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$" | sort | head -n $head | tail -n $tail)
          blk_cnt=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$" | head -n $head | tail -n $tail | wc -l)
      fi
      
  else
      if [ "{{ use_uhp }}" = "true" ]; then

          # Exclude, /dev/oracleoci/oraclevdb from the list, since it will be used for UHP volume.
          # Gather list of block devices
          blk_lst=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$" | sort | head -n $head | tail -n $tail)
          blk_cnt=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | egrep -v "vdb$" | head -n $head | tail -n $tail | wc -l)
      else
          # Gather list of block devices
          blk_lst=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | sort | head -n $head | tail -n $tail)
          blk_cnt=$(ls  /dev/oracleoci/oraclevd*  | grep -v vda[0-9] | grep -v vda$ | grep -v nvme | head -n $head | tail -n $tail | wc -l)
      fi
  
  fi
}



configure_disks()
{
  if [ "$use_uhp" = "true" ]; then
      mount_point="/mnt/${fs0_name}"
      find_UHP_block_volume
      process_disks "$blk_lst" "disk0"
  fi
   
  if [ "{{ fs_type }}" = "Persistent" ]; then

    if [ "$use_non_uhp_fs1" = "true" ]; then
        mount_point="/mnt/${fs1_name}"
        find_block_volumes "1" "$fs1_disk_count"
        process_disks "$blk_lst" "disk"
    fi

    if [ "$use_non_uhp_fs2" = "true" ]; then
        mount_point="/mnt/${fs2_name}"
        start_position=$((fs1_disk_count + 1))
        find_block_volumes "$start_position" "$fs2_disk_count"
        process_disks "$blk_lst" "disk2"
    fi

    if [ "$use_non_uhp_fs3" = "true" ]; then
        mount_point="/mnt/${fs3_name}"
        start_position=$((fs1_disk_count + fs2_disk_count + 1))
        find_block_volumes "$start_position" "$fs3_disk_count"
        process_disks "$blk_lst" "disk3"
    fi

 

    ##find_block_volumes
    # call function
    ##process_disks "$blk_lst"

  else

    # nvme_lst will be full path, eg  /dev/nvme0n1 ,etc
    nvme_lst=$(ls /dev/* | grep nvme | grep n1 | sort)
    nvme_cnt=$(ls /dev/* | grep nvme | grep n1 | wc -l)

    if [ $nvme_cnt -gt 0 ]; then
      # call function
      process_disks "$nvme_lst"
    else
    
    ## TODO - update to support multiple xfs for single node.
      find_block_volumes
      # call function
      process_disks "$blk_lst"
    fi

  fi
}




##################
# Start of script
##################

set -x

use_uhp={{ use_uhp }}
use_non_uhp_fs1={{ use_non_uhp_fs1 }}
use_non_uhp_fs2={{ use_non_uhp_fs2 }}
use_non_uhp_fs3={{ use_non_uhp_fs3 }}

# fs0 reserverd for uhp
fs0_name={{ fs0_name }}
fs0_disk_count=1

fs1_name={{ fs1_name }}
fs1_disk_count={{ fs1_disk_count }}

fs2_name={{ fs2_name }}
fs2_disk_count={{ fs2_disk_count }}


fs3_name={{ fs3_name }}
fs3_disk_count={{ fs3_disk_count }}




coreIdCount=`grep "^core id" /proc/cpuinfo | sort -u | wc -l`
socketCount=`echo $(($(grep "^physical id" /proc/cpuinfo | awk '{print $4}' | sort -un | tail -1)+1))`
if [ $((socketCount*coreIdCount)) -ge 24  ]; then
  sed -i "s/#RPCNFSDCOUNT=.*/RPCNFSDCOUNT=16/g" /etc/sysconfig/nfs
  # The above change require restart of systemctl restart  nfs-server, which will happen as part of NFS setup.
  # To change the number of nfsd threads without restarting services to say 16, run the command: rpc.nfsd 16, but this change will be lost after nfs-server restart
fi


# block_size is expected be to numerical only, but still check and remove k,K,kb,KB them, if they exist.
lvm_stripe_size=`echo {{ block_size }} | gawk -F"k|K|KB|kb" ' { print $1 }'` ;
echo $lvm_stripe_size;


configure_disks

# required for quota information to be sent from server to clients - read only.
systemctl enable rpc-rquotad
systemctl start rpc-rquotad


# configure NFS exports for standalone NFS server.  If its NFS with HA, the logic is written in install_ha_config.sh script.
if [ "{{ fs_ha }}" = "true" ]; then

  echo "NFS config logic in install_ha_config.sh file, will be called later in deployment"

else

  echo "non-HA setup..."
  SSH_OPTIONS=" -i /home/opc/.ssh/id_rsa -o BatchMode=yes -o StrictHostkeyChecking=no "
  MDATA_VNIC_URL="http://169.254.169.254/opc/v1/vnics/"
  lv_name="{{ lv_name }}"
  vg_name="{{ vg_name }}"
  filesystem_name="nfsshare"
  filesystem_type="xfs"
  filesystem_device="/dev/{{ vg_name }}/{{ lv_name }}"
  filesystem_dir="/mnt/nfsshare"
  exportfs_client_spec_1_cidr_block="{{ private_storage_subnet_cidr_block }}"
  exportfs_client_spec_2_cidr_block="{{ public_subnet_cidr_block }}"

  exportfs_client_spec_1_netmask=`ipcalc -m  $exportfs_client_spec_1_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_1_network=`ipcalc -n  $exportfs_client_spec_1_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_2_netmask=`ipcalc -m  $exportfs_client_spec_2_cidr_block | gawk -F "=" '{ print $2}'`
  exportfs_client_spec_2_network=`ipcalc -n  $exportfs_client_spec_2_cidr_block | gawk -F "=" '{ print $2}'`

  exportfs_client_spec_1="${exportfs_client_spec_1_network}/${exportfs_client_spec_1_netmask}"
  exportfs_client_spec_2="${exportfs_client_spec_2_network}/${exportfs_client_spec_2_netmask}"

  if [ "{{ storage_server_dual_nics }}" = "true" ]; then
    exportfs_client_spec_3_cidr_block="{{ private_fs_subnet_cidr_block }}"
    exportfs_client_spec_3_netmask=`ipcalc -m  $exportfs_client_spec_3_cidr_block | gawk -F "=" '{ print $2}'`
    exportfs_client_spec_3_network=`ipcalc -n  $exportfs_client_spec_3_cidr_block | gawk -F "=" '{ print $2}'`
    exportfs_client_spec_3="${exportfs_client_spec_3_network}/${exportfs_client_spec_3_netmask}"
  fi

  options="rw,sync,no_root_squash,no_all_squash,no_subtree_check,insecure_locks"
  nfs_root_dir="${filesystem_dir}/exports"

  mv /etc/exports /etc/exports.backup
# it uses fsid values.  If fsid are used, then the client using NFSv4, need to use / to mount fsid=0 root and not /mnt/nfsshare/exports. Client code is currently using format: /mnt/nfsshare/exports also for NFSv4 for HA and non-HA.
#  echo "${nfs_root_dir} ${exportfs_client_spec_1}(${options},fsid=0) ${exportfs_client_spec_2}(${options},fsid=0) ${exportfs_client_spec_3}(${options},fsid=0) " >> /etc/exports

  if [ "{{ storage_server_dual_nics }}" = "true" ]; then
    echo "${nfs_root_dir} ${exportfs_client_spec_1}(${options}) ${exportfs_client_spec_2}(${options}) ${exportfs_client_spec_3}(${options}) " >> /etc/exports
  else
    echo "${nfs_root_dir} ${exportfs_client_spec_1}(${options}) ${exportfs_client_spec_2}(${options}) " >> /etc/exports
  fi

  sudo systemctl restart nfs-server

fi
